### 1、基础知识
- 英语
```
gradient descent 梯度下降

```
- 滤波
低频分量：指图像中强度（亮度/灰度）变化比较平缓的部分。
高频分量：指图像中强度（亮度/灰度）变化比较强的部分。
低通滤波：就是去掉高频信号，留下低频信号，这就是低通滤波。
高通滤波：就是去掉低频信号，留下高频信号，这就是高通滤波。
- sobel算子
![[Pasted image 20241230193627.png]]
用于找到图像中灰度变化较大的地方，通常表现为图像中的边缘或轮廓。效果如下
![[Pasted image 20241230193718.png]]
- 随机梯度下降算法SGD/MBGD
损失函数中存在变量，而这些变量就是要训练的参数。训练到什么时候为止呢？损失函数变化小于某个数值。对于随机梯度下降算法，就是先定义损失函数单个数据点的损失函数，再初始化参数，
然后训练集进行分批，针对每一个批进行如下操作，初始化参数与训练集中的数据，就可以计算出当下损失函数值，然后再变量替代具体参数数值，求导，更新参数，计算新的损失函数值，比较，看是否前后相差小于某个值，如果小，则说明收敛。如果大，则进行下一个批次。对训练集进行多次相同的操作，叫做epoch，对每个批进行调整，就是iteration，batch的大小就是batchsize
损失函数是一个只包含要训练参数的函数。如果是对于BGD来讲，函数在整个过程中是不变的。但是在SGD中是会变的，但是变量在不同的批次中共享，因此，每个batch需要计算损失函数，并且需要对计算梯度。

- 批量梯度下降BGD 
- 训练集，验证集，测试集
### 2、对神经网络中各个层的认识
卷积层
池化层
归一化层
- 01归一化
![[Pasted image 20241230200826.png]]
- .正态分布归一化
![[Pasted image 20241230200916.png]]
flatten层
全连接层


神经网络的训练过程：
- 获取features和targets对应的张量，从而得到数据集，进而得到加载器
- 定义模型：这里需要注意切换视角，对于单个神经元而言，需要考虑的是接受多少个维度的输入，输出是确定的一个，这一层的神经元而言，考虑的是神经元个数。有这两个维度，就能确定一层。