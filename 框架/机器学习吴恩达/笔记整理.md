### 知识点的逻辑梳理
对于一个问题，可以根据训练集和输出值得特征进行分类，找到适合此类问题的算法。对于算法，统一的模式就是先确定一个模型，针对模型确定代价函数，利用梯度下降算法求出使得代价函数最小的参数，最后利用该模型对新数据进行预测。
为了能够顺利地开展以上的模式，对训练集的操作有进行特征缩放，增加或删除特征，增加训练数据，对代价函数的操作有在使得代价函数有意义的情况下，增加新的代价函数类别(比如交叉桑函数)，使用正则化。对代码的优化，有向量化。
在以上的操作中，基于了很多目的，比如为了防止过拟合，对训练集增加或删除特征，增加训练数据，对代价代数进行正则化。为了梯度下降算法提高计算效率，特征的尺度放缩，向量化。为了能让梯度下降算法能够使用，利用了交叉熵函数。
不同的问题应当使用不同的模型，对于回归问题，利用线性回归模型和平方差代价函数；对于分类问题，利用sigmoid函数和交叉熵函数。
以上都是机器学习的思想，但是最终的目的还是要代码说话，在代码层面，问题还是很多的。numpy进行向量化的表示，matplotlib进行画图，python的语法，pytorch框架的使用，这些都是我面临的问题。
此外，还有神经网络对传统的模式产生了深远的影响。神经网络的模式是怎样的呢？
==待定：模型转变成了神经网络了，但是要对最后输出层进行一定的操作变换。有了框架，不需要手动地进梯度下降算法了。==
### 通识类
#### 1.机器学习的步骤：
- 训练：训练集（training set）在指定的学习算法（learning algorithm）下，训练模型f。
- 预测：向f种输入特征x，得到预测是y_hat
#### 2.机器学习的算法的分类的依据：
- 根据对训练集是否包含target，将算法分为了监督学习和无监督学习
- 在监督学习中，根据输出的结果是否有限，分为回归和分类
- 注：以上的分类都是对特定一个算法的部分特征进行分类的，不涉及具体的算法。
#### 3.比较通用的机器学习的流程：
#### 4.对代价函数和loss函数的理解
含义/作用：在整个训练集上，模型的预测值与真实值之间的量度。数值越小，模型拟合得越好，反之越差。
目标：找到模型得参数，使得代价函数最小
思维模式：模型是什么？模型中的参数是哪儿写？代价函数是什么？怎样对参数取值，能使得代价函数最小？
![[Pasted image 20240521211343.png]]
对loss函数的理解：
含义：表征一个样本点预测值与真实值直接的差距。
要求：正值，差距越大，数值越大
loss函数与代价函数之间的关系：
loss函数的均值就是代价函数。
#### 5.对梯度下降算法的理解：
##### 5.1作用/含义：使得代价函数最小的算法。
##### 5.2学习率的理解：
###### 4.1.1学习率的本质
确定方向后，跨的步子的大小
###### 4.1.2学习率的大小对梯度下降算法的影响
- 学习率太大：overshoot,diverge
![[Pasted image 20240521213345.png]]
- 学习率太小:梯度下降太慢
![[Pasted image 20240521213305.png]]

##### 5.3偏导数的理解：
本质：站在给定的参数点上，寻找一个下降最快的方向，也就是梯度的反方向。
##### 5.4图像
![[Pasted image 20240521214200.png]]
一个人不断地寻找方向，跨步子，寻找方向，跨步子，直到到达山底。
##### 5.5 梯度下降算法的执行步骤：
![[Pasted image 20240521214413.png]]
步骤二中，更新的函数
![[Pasted image 20240521212328.png]]
##### 5.6 通过看代价函数随迭代函数的变化判断梯度下降是否收敛
创建一个图，横坐标是梯度下降算法的执行次数，纵坐标是代价函数的值。可以认为两次迭代的过程中代价函数的变化值小于某种程度，就认为可以收敛。
![[Pasted image 20240522151739.png]]
图像可能的情况：
![[Pasted image 20240522152316.png]]
若是图像一，说明学习率太大了
![[Pasted image 20240522152502.png]]
若是图像二，说明公式中的减号改成了加号：
![[Pasted image 20240522152537.png]]
太大太小刚刚好的三三者对比
![[Pasted image 20240522152754.png]]
选择学习率的原则：
![[Pasted image 20240522152817.png]]

#### 6.向量化
##### 一些向量化的操作
- 点乘
- 加减
##### 向量化的表示方法（包括通用与numpy中的表示）
###### 对数据本身的向量化
以对参数和特征的向量化表示为例
通用：
![[Pasted image 20240521220757.png]]
numpy：
![[Pasted image 20240521221204.png]]
###### 操作之点乘
![[Pasted image 20240521222157.png]]
![[Pasted image 20240521222216.png]]
###### 操作之加减
![[Pasted image 20240521222319.png]]
![[Pasted image 20240521222328.png]]


##### 理解向量化的意义
###### 简化代码
以计算以下式子为例：
![[Pasted image 20240521220927.png]]
没有向量化：
![[Pasted image 20240521221008.png]]
有向量化：
![[Pasted image 20240521221031.png]]
##### 使能够并行计算，提高代码的运行效率
- 点乘
![[Pasted image 20240521221302.png]]
没有向量化的时候，每一步`wi*xi` 是顺序计算（效率低）
向量化的时候，点乘时，`wi*xi` 是并行计算（效率高）
- 加减
![[Pasted image 20240521222015.png]]
#### 7.特征缩放（feature scaling）
##### 实现特征缩放的算法
###### 范围除以最大尺度
![[Pasted image 20240522150442.png]]

###### Mean normalization
![[Pasted image 20240522150614.png]]
平均值再零点，点的分布比较均匀

###### Z-score normalization
![[Pasted image 20240522151113.png]]

##### 需要尺度缩放的情形

![[Pasted image 20240522151203.png]]
##### 理解特征缩放的意义
意义：使得梯度下降算法运行得更快
效果：无论尺度的大小，都能将其映射到一个差不多的范围中。
定性解释：
有两个尺度的特征，一个是房子的面积（300~2000），一个是房间的数量（0~5），作为target 房屋价格的影响因素。可见这两个特征的尺度差距很大。特征的尺度越大，修饰该尺度的参数就会越小，其关系如下图所示。
![[Pasted image 20240521225635.png]]
所以对特征画散点图和对参数画等高线图如下：
![[Pasted image 20240521225832.png]]
画的散点图非常的密集，近乎于一条直线。
画的登高线图，w1的取值范围和w2的取值范围也不是同一个尺度就会很狭长。
进行梯度下降算法：
![[Pasted image 20240521230107.png]]
确定一个方向后，再乘以一个学习率，两个方向的的变化值都不会特别大。对于取值范围小的w，尽管这个方向的变化值不大，但是由于本来基数就小，所以影响比较大。而对于取值范围大的来讲，这个范围小，分母的基数还大，几乎没啥变化。这就导致了上图右边图像的现象。很明显这种梯度下降的收敛速度很慢。
通过一定的手段，将x1和x2缩放到差不多的尺度后，将会出现以下情况：
![[Pasted image 20240522150111.png]]





#### 8.特征工程
![[Pasted image 20240522152940.png]]

#### 9.对一些模型,以及基于模型的特点，对代价函数的推导
##### 线性函数 f=wx+b
建模理由：认为所有的点(features,targets)都在这个线性空间上。
代价函数的推导：
误差越大，代价函数越大？
- 对于每个点来讲，误差就是(fw,b(x)-y)^2
- 对于所有点来讲，平均每个点的误差就是
![[Pasted image 20240522161014.png]]
代价函数能否利用梯度下降算法求得极值点？
事实证明，在线性函数的模型下的代价函数一定时凸函数，所以可以。
##### sigmoid function与决策边界
#### 对过拟合问题的解决
##### 概念与情形
high bias<-->underfitt
high variance<-->overfit
just right
- 回归问题
![[Pasted image 20240522164333.png]]
- 分类问题
![[Pasted image 20240522164343.png]]

###### 模型的特点
建模理由：对每个点是1的可能性进行判断
函数的特点：函数值介于0到1之间，用来表征预测值为1的概率。在z=fw,b(x)，z大于0,概率大于1/2。z小于0概率小于1/2,其中fw,b(x)=0就是边界函数。
适用范围：二分类问题
sigmoid function:
![[Pasted image 20240522154410.png]]
decesion boundary：

![[Pasted image 20240522154912.png]]
![[Pasted image 20240522154930.png]]

###### 代价函数的推导
- 对延续使用线性函数中的用到代价函数可行的推导
问题一：代价函数能够表征预测值和真实值的接近程度？能！
对于每个点来讲，误差就是(fw,b(x)-y)^2,当y的真实值是1时，预测值越大，误差越小;当真实值为0时，预测值越大，误差越大。所以他是可以预测值和真实值的接近程度的。
问题二：代价函数是否为凸函数，也就是能够方便地利用梯度下降算法求得极小值？不能！
![[Pasted image 20240522161646.png]]
- 对新的代价函数地探究
![[Pasted image 20240522162024.png]]
对loss函数的简化如下方程：
![[Pasted image 20240522162845.png]]
注意：对于一个样本的loss函数取值为，真实值为某标签时，模型基于特征对这个真实值的概率的负对数。
问题一：代价函数能够表征预测值和真实值的接近程度？能！
由于概率的取值时0~1的，负对数函数在这个区间时单调递减且为正数的。所以概率越大，损失函数越小，反之。
问题二：能否梯度下降方便计算极小值？能！ 
##### 对过拟合问题的解决
###### 增加更多的training examples
![[Pasted image 20240522164454.png]]

###### 对特征进行选择：增加或删除特征
![[Pasted image 20240522164621.png]]

###### 正则化：降低参数的大小
![[Pasted image 20240522164739.png]]

##### 详解正则化
![[Pasted image 20240522165023.png]]
目的：使得包含正则化项的代价函数最小化
对于第一项：使得模型拟合训练集的数据
对于第二项：使得模型中的参数不能太大
对于regularization parameter λ：均衡第一项和第二项
- 当λ太大时，使得参数不能太大的因素占主导，导致所有的w都是0，就只剩下b来拟合了
- 当λ太小时，使得第二项几乎不存在，使得让模型拟合数据占主导，就出现了过拟合了。
### 符号
training set：包含features和targets

![[Pasted image 20240521204632.png]]
![[Pasted image 20240522153319.png]]
y_hat: y的预测值
矩阵：大写字母表示
向量和标量：小写字母表示
### 监督学习
1.定义：learns from being given "right answer",通过的来讲，训练集既给features也给targets
2.应用：判断邮件是否为垃圾邮件，翻译，语音识别
3.两种监督学习的两种分类：
- 回归（regression）：
	- 定义：根据特征预测一个值，这个值有无限种可能
	- 举例：根据房子的面积，预测房子的售价
	- 算法：
		- univarate linear regression 单变量的线性回归 f=wx+b
- 分类（classification）:
	- 定义：根据特征预测一种类别，类别在有限种可能中
	- 举例：根据肿瘤的大小，判断肿瘤是恶性的还是良性的
- 这两种类别的差同：
	- 同：在训练的时候，都既给定了特征也给定了值
	- 差：回归的输出的可能是无限个，分类的输出是有限个
### 具体的算法
#### univarate linear regression
##### 1.符号与名称
 - slope:斜率w
 - intercept:截距b
 - alpha:学习率
#####  2.流程与步骤
 2.1确定模型:y=wx+b，注意参数是w和b
 2.2 确定代价函数：squared error cost function 
 - 函数
 ![[Pasted image 20240521210707.png]]
- 图像
![[Pasted image 20240521211647.png]]

2.3 梯度下降算法
![[Pasted image 20240521215827.png]]
##### 3.使用案例
根据房子的面积预测房子的价格
#### multiple linear regression
![[Pasted image 20240521222813.png]]
![[Pasted image 20240521222945.png]]
使用案例：
根据房子的面积，房间的数量，房屋的年限来预测房子的价格

### 神经网络
